{
  "article.txt": "At GitHub we place an emphasis on stability, availability, and performance. A large component of ensuring we excel in these areas is deploying services on bare-metal hardware. This allows us\u2026 At GitHub we place an emphasis on stability, availability, and performance. A large component of ensuring we excel in these areas is deploying services on bare-metal hardware. This allows us to tailor hardware configurations to our specific needs, guarantee a certain performance profile, and own the availability of our systems from end to end. Of course, operating our own data centers and managing the hardware that\u2019s deployed there introduces its own set of complications. We\u2019re now tasked with tracking, managing, and provisioning physical pieces of hardware \u2014 work that is completely eliminated in a cloud computing environment. We also need to retain the benefits that we\u2019ve all come to know and love in cloud environments: on-demand compute resources that are a single API call away. Enter gPanel, our physical infrastructure management application. gPanel is a Ruby on Rails application that we started developing over three years ago as we were transitioning from a managed environment to our own data center space. It was identified early on that we\u2019d need the ability to track physical components of our new space; cabinets, PDUs, chassis, switches, and loose pieces of hardware. With this in mind, we set out building the application. As we started transitioning hosts and services to our own data center, we quickly realized we\u2019d also need an efficient process for installing and configuring operating systems on this new hardware. This process should be completely automated, allowing us to make it accessible to the entire company. Without this, specific knowledge about our new environment would be required to spin up new hosts, which leaves the very large task of a complete data center migration exclusively in the hands of our small Operations team. Since we\u2019d already elected to have gPanel act as the source of truth for our data center, we determined it should be responsible for server provisioning as well. The system we ended up with is overall pretty straight-forward and simple \u2014 goals for any of our new systems or software. We utilize a few key pieces to drive the entire process. Our hardware vendor configures machines to PXE boot from the network before they arrive at our data center. Machines are racked, connected to our network, and powered on. From there, our DHCP/PXE server tells the machines tochainload iPXE and then contact gPanel for further instructions. gPanel can identify the server (or determine that it\u2019s brand new) with the serial number that\u2019s passed as a parameter in the iPXE request. gPanel defines a number of states that chassis are in. This state is passed to our Ubuntu PXE image via kernel parameters so it can determine which action to take. These actions are driven by a simple set of bash scripts that we include in our Ubuntu image. The initial state isunknownwhere we simply collect data about the machine and record it in gPanel. This is accomplished using Facter for gathering system information, exporting it as JSON, and then POSTing it to gPanel\u2019s API. gPanel has a number of jobs that process this JSON and create the appropriate records. We try to model as much as possible in the application; CPUs, DIMMs, RAID cards, drives, NICs, and more are all separate records in the database. This allows us to track parts as they\u2019re replaced, moved to a different machine, or removed entirely. Once we\u2019ve gathered all the information we need about the machine, we enterconfiguring, where we assign a static IP address to the IPMI interface and tweak our BIOS settings. From there we move tofirmware_upgradewhere we update FCB, BMC, BIOS, RAID, and any other firmware we\u2019d like to manage on the system. At this point we consider the initial hardware configuration complete and will begin the burn-in process. Our burn-in process consists of two states in gPanel;breakinandmemtesting.breakinuses a suite fromAdvanced Clusteringto exercise the hardware and detect any problems. We\u2019ve added a script that POSTs updates to gPanel throughout this process so it can determine whether we have failures or not. If a failure is detected, the chassis is moved to ourfailedstate where it sits until we have a chance to review the logs and replace the bad component. If the chassis passesbreakin, we\u2019ll move on tomemtesting. Inmemtestingwe boot a customMemTest86image and monitor it while it completes a full pass. Our custom version of MemTest86 changes the color of the failure message to red which allows us to detect trouble. We\u2019ve hacked together a Ruby script that retrieves a console screenshot via IPMI and checks the color in the image to determine if we\u2019ve hit a failure or not. Again, if a failure is detected, we\u2019ll transition the chassis tofailed, otherwise it moves on toready. Thereadystate is where our available pool of machines will sit until someone comes along and brings it into production. Once machines have completed the burn-in process and deemed ready for production service, a user can instruct gPanel to install an operating system. Like the majority of our tooling, this is driven viaHubot, our programmable chat bot. First, the user will need to determine which chassis they\u2019d like to perform the installation on. Once the chassis is selected, you can initiate the installation. If the user needs a different RAID configuration, or to have the host brought up on a different Puppet branch, they can specify those with the install command as well. If we\u2019re looking to spin up a number of hosts to expand capacity for a certain service tier, we can instruct gPanel to do this with ourbulk-installcommand. This command takesapp,role,chassis_type, andcountparameters, selects the appropriate hardware from ourreadypool, and initiates the installations. At this point gPanel will transition the chassis to ourinstallingstate and reboot the machine via IPMI. In this state we PXE boot the Ubuntu installer and retrieve apreseed configurationfrom gPanel. This configuration is rendered dynamically based on the hardware configuration and the options the user provided in their install command. Once the installation is complete, we move to theinstalledstate where gPanel will instruct machines to boot from their local disk. When we\u2019re ready to decommission a host we simply tell Hubot, who will ask for confirmation in the form of a \u201cmagic word\u201d. gPanel transitions the chassis back to ourreadystate and makes it available again for future installations. We\u2019ve been pleased with the ease at which we\u2019re able to bring new hardware into the data center and make it available to the rest of the company. We continue to find room for improvement and are constantly working to further automate the procurement and provisioning process. @leereilly Senior Program Manager, GitHub Developer Relations. Open source hype man, AI whisperer, hackathon and game jam wrangler. I write && manage programs, support dev communities, and occasionally ship something. GitHub is introducing post-quantum secure key exchange methods for SSH access to better protect Git data in transit. Our best practices for quickly identifying, resolving, and preventing issues at scale. Plus, considerations in updating one of GitHub\u2019s oldest and most heavily used features. Everything you need to master GitHub, all in one place. Build what\u2019s next on GitHub, the place for anyone from anywhere to build anything. Meet the companies and engineering teams that build with GitHub. Catch up on the GitHub podcast, a show dedicated to the topics, trends, stories and culture in and around the open source developer community on GitHub. Discover tips, technical guides, and best practices in our biweekly newsletter just for devs.",
  "comments.json": "[\n  {\n    \"id\": 10661583,\n    \"author\": \"sargun\",\n    \"text\": \"This seems wrong to me. This was the state of the art ~3 years ago. Now, I feel like all of the machines should be provisioned already with an OS, and a basic image, and a orchestration system like CoreOS / Mesos / Docker should specialize them.IMHO, requiring hardware, or the entire machine should be exception, not the rule.\",\n    \"points\": null,\n    \"time\": \"2015-12-02T07:51:15.000Z\",\n    \"children\": [\n      {\n        \"id\": 10661594,\n        \"author\": \"packetized\",\n        \"text\": \"Wait, why? Are you advocating for the use of an abstraction layer where there isn't always a business case for using one?\",\n        \"points\": null,\n        \"time\": \"2015-12-02T07:54:04.000Z\",\n        \"children\": []\n      },\n      {\n        \"id\": 10661634,\n        \"author\": \"detaro\",\n        \"text\": \"Just because you use Containers/VMs for most of your apps doesn't mean that the lower levels don't need attention: installing OSes in the first place, hardware testing (both initially and to identify defects later), ...And for important fileservers and databases you're going to run on specific hardware for a long time.\",\n        \"points\": null,\n        \"time\": \"2015-12-02T08:10:19.000Z\",\n        \"children\": []\n      },\n      {\n        \"id\": 10661642,\n        \"author\": \"kbar13\",\n        \"text\": \"you realize the cloud runs on hardware right. machines don't magically come with everything installed and configured.\",\n        \"points\": null,\n        \"time\": \"2015-12-02T08:12:09.000Z\",\n        \"children\": []\n      },\n      {\n        \"id\": 10661665,\n        \"author\": \"ownagefool\",\n        \"text\": \"Even if you were going to run CoreOS or Mesos on the machine, you'd still manage it booting your specific image, which you can change, rather than trusting the pre-installed dell verion and managing that relationship.Now there's probably some room for debate on whether these guys job should just be outsourced to Amazon, but github has some pretty good uptime and they seem to know what they're doing, thus they've probably already won that debate.\",\n        \"points\": null,\n        \"time\": \"2015-12-02T08:19:46.000Z\",\n        \"children\": []\n      },\n      {\n        \"id\": 10661677,\n        \"author\": \"lwhalen\",\n        \"text\": \"Sometimes you have a workload that really isn't a fit for virtualization/containers/whatever the latest Rails hotness is, at all, and you just need to throw a couple of cargo trailers of insanely massively-spec'd servers at the problem. In those cases, your 'old school' server provisioning toolkit had better be on-point.It's easy to forget just how ridiculously powerful bare iron is these days. Go to Dell.com and see how much RAM you can cram into a U or three or four today in 2015. Or see how many IOPS a modern NetApp or Symmetrix (EMC) can push with 'flashcache' or million-dollar SSDs. It is ridiculous, and while a lot of those platforms are meant for 'building your own private cloud', etc, there's a non-trivial amount of workloads/projects where bare-iron is the best tool for the job.\",\n        \"points\": null,\n        \"time\": \"2015-12-02T08:22:51.000Z\",\n        \"children\": [\n          {\n            \"id\": 10661823,\n            \"author\": \"icebraining\",\n            \"text\": \"Containers are just a namespacing tool, though; you're still running on bare metal (well, bare Linux). Docker in particular runs on AuFS, which is slow, but other containerization tools just use a chroot.\",\n            \"points\": null,\n            \"time\": \"2015-12-02T09:07:08.000Z\",\n            \"children\": [\n              {\n                \"id\": 10662187,\n                \"author\": \"vidarh\",\n                \"text\": \"Docker can run on any number of things, include btrfs and overlayfs+ext4, as well as devicemapper. E.g. CoreOS defaults to overlayfs.\",\n                \"points\": null,\n                \"time\": \"2015-12-02T10:47:08.000Z\",\n                \"children\": []\n              }\n            ]\n          },\n          {\n            \"id\": 10662335,\n            \"author\": \"mariusmg\",\n            \"text\": \">Sometimes you have a workload that really isn't a fit for virtualizationYeah, any serious I/O load is unsuited for virtualization.\",\n            \"points\": null,\n            \"time\": \"2015-12-02T11:28:49.000Z\",\n            \"children\": [\n              {\n                \"id\": 10662530,\n                \"author\": \"devonkim\",\n                \"text\": \"A lot of other latency-sensitive applications tend to have so many adverse performance conditions (that can usually be remediated with a lot of blood sweat and tears) under virtualization that it becomes easier to just go bare metal and deal with physical infrastructure overhead.\",\n                \"points\": null,\n                \"time\": \"2015-12-02T12:29:47.000Z\",\n                \"children\": []\n              }\n            ]\n          }\n        ]\n      },\n      {\n        \"id\": 10661743,\n        \"author\": \"mverwijs\",\n        \"text\": \"I'm surprised everyone is still installing to disk. In 2008/2009, we had a POC where we ran the OS from memory after booting over PXE. * boot a live image into memory * point LXD/RKT/Docker to /containers * ... * profit!\",\n        \"points\": null,\n        \"time\": \"2015-12-02T08:40:59.000Z\",\n        \"children\": [\n          {\n            \"id\": 10661797,\n            \"author\": \"iofj\",\n            \"text\": \"The big issue I'm having with that is that it involves trusting vendors to get network boot right. Especially when it comes to the looping part of \\\"loop until DHCP gets a response\\\" it becomes a problem. One of the cheap vendor tries 30 times and then goes to a boot failed screen after trying the disk.Also, 1 time out of a 4-5000 or so network boot fails. Not sure why.\",\n            \"points\": null,\n            \"time\": \"2015-12-02T08:58:23.000Z\",\n            \"children\": [\n              {\n                \"id\": 10662162,\n                \"author\": \"vidarh\",\n                \"text\": \"If you have IPMI on the server this doesn't become such a big problem - you can reasonably trigger resets/reboots if it's not up after a given amount of time.\",\n                \"points\": null,\n                \"time\": \"2015-12-02T10:40:08.000Z\",\n                \"children\": [\n                  {\n                    \"id\": 10670315,\n                    \"author\": \"iofj\",\n                    \"text\": \"We buy the cheapest server that meets our needs, and buy it in somewhat larger quantities (often double what was originally envisioned for less than was originally budgeted). Much more efficient.But it does mean no IPMI. However I built a small circuit that sits on a power cable that can interrupt said power cable with a relais that sits on a bus plugged into our server, so we can do the reboot thing.I've been meaning to redo that power cable circuit using wifi as the linking technology, now that we have esp8266 available.\",\n                    \"points\": null,\n                    \"time\": \"2015-12-03T16:05:42.000Z\",\n                    \"children\": []\n                  }\n                ]\n              },\n              {\n                \"id\": 10662632,\n                \"author\": \"mverwijs\",\n                \"text\": \"That's where iLO comes in. iLO is horrible, but you can ssh to it and set all manner of stuff.When we didn't have PXE, we had a script that told iLO to boot from CD, and that the CD was located at http://something/bootme.iso. iLO would always have network, and would pass the .iso magically to the server as device to boot from.\",\n                \"points\": null,\n                \"time\": \"2015-12-02T12:57:08.000Z\",\n                \"children\": []\n              }\n            ]\n          },\n          {\n            \"id\": 10661879,\n            \"author\": \"bcantrill\",\n            \"text\": \"For whatever it's worth, SmartDataCenter, Joyent's open source SmartOS-based system for operating a cloud[1], does exactly this[2] -- and (as explained in an old but still technically valid video[3]) we have found it to be a huge, huge win. And we even made good on the Docker piece when we used SDC as the engine behind Triton[4] -- and have found it all to be every bit as potent as you suggest![5][1] https://github.com/joyent/sdc[2] https://github.com/joyent/sdc/blob/master/docs/developer-gui...[3] https://www.youtube.com/watch?v=ieGWbo94geE[4] https://www.joyent.com/blog/triton-docker-and-the-best-of-al...[5] https://www.joyent.com/blog/spin-up-a-docker-dev-test-enviro...\",\n            \"points\": null,\n            \"time\": \"2015-12-02T09:24:05.000Z\",\n            \"children\": []\n          },\n          {\n            \"id\": 10662853,\n            \"author\": \"vidarh\",\n            \"text\": \"CoreOS lets you do this if you PXE boot. It will then by default run entirely out of RAM.\",\n            \"points\": null,\n            \"time\": \"2015-12-02T13:46:11.000Z\",\n            \"children\": []\n          }\n        ]\n      },\n      {\n        \"id\": 10662100,\n        \"author\": \"jon-wood\",\n        \"text\": \"I built something similar for a managed hosting provider ~10 years ago. That doesn't make it any less useful now, and this does a ton more than our tool did, far more elegantly.At some point someone needs to manage the actual hardware, whether that's you, or a middleman, and when you're handling hundreds or even thousands of devices its just not practical without automation.\",\n        \"points\": null,\n        \"time\": \"2015-12-02T10:25:36.000Z\",\n        \"children\": []\n      },\n      {\n        \"id\": 10663025,\n        \"author\": \"jssjr\",\n        \"text\": \"GitHub's physical infrastructure team doesn't dictate what technologies our engineers can run on our hardware. We are interested in providing reliable server resources in a easily consumable manner. If someone wants to provision hardware to run docker containers or similar, that's great!We may eventually offer higher order infrastructure or platform services internally, but it's not our current focus.\",\n        \"points\": null,\n        \"time\": \"2015-12-02T14:19:20.000Z\",\n        \"children\": []\n      }\n    ]\n  },\n  {\n    \"id\": 10661606,\n    \"author\": \"alpb\",\n    \"text\": \"> We've hacked together a Ruby script that retrieves a console screenshot via IPMI and checks the color in the image to determine if we've hit a failure or not.That's pretty funny yet sounds a lot familiar to many of us as every now and then we all do these sort of nasty hacks.\",\n    \"points\": null,\n    \"time\": \"2015-12-02T07:59:11.000Z\",\n    \"children\": [\n      {\n        \"id\": 10661621,\n        \"author\": \"markild\",\n        \"text\": \"Also probably a lot better than to make MemTest86 do all sorts of network stuff.\",\n        \"points\": null,\n        \"time\": \"2015-12-02T08:05:25.000Z\",\n        \"children\": []\n      },\n      {\n        \"id\": 10661716,\n        \"author\": \"versteegen\",\n        \"text\": \"Yikes, having attended a computer vision conference last week, that sort of apporach is actually starting to sound reasonable and intelligent!\",\n        \"points\": null,\n        \"time\": \"2015-12-02T08:35:23.000Z\",\n        \"children\": []\n      },\n      {\n        \"id\": 10661780,\n        \"author\": \"shanselman\",\n        \"text\": \"Ya, I came here to talk about that line as well. The whole article was interesting, but that line stood out as a pretty surprising hack for 2015.\",\n        \"points\": null,\n        \"time\": \"2015-12-02T08:52:46.000Z\",\n        \"children\": []\n      },\n      {\n        \"id\": 10662849,\n        \"author\": \"osxrand\",\n        \"text\": \"Hah! I wrote a script a few months back and had to solve an issue of figuring out the state of a program running under wine, and that was my solution (to not in ruby, just a quick bash script). I was pretty happy with the results but if felt like an incredibly crude way to solve the problem, now I read that this is being used at much higher levels than I'll get to, maybe it's not so bad :)\",\n        \"points\": null,\n        \"time\": \"2015-12-02T13:46:07.000Z\",\n        \"children\": []\n      },\n      {\n        \"id\": 10663179,\n        \"author\": \"SEJeff\",\n        \"text\": \"Vs just buying opengear console servers and something like conserver to get the actual text via serial like most large Unix environments (every job I've worked at) do. Then you can just scrape the text.\",\n        \"points\": null,\n        \"time\": \"2015-12-02T14:44:25.000Z\",\n        \"children\": [\n          {\n            \"id\": 10664650,\n            \"author\": \"FireBeyond\",\n            \"text\": \"Right, this surprised me. They're doing IPMI, so why not SOL (Serial over LAN) to get the raw stream?Actually, on second thoughts, I'm not surprised so much. I've read many threads on Supermicro IPMI and people's frustration with it (reliant on outdated Java, and hacked together wrappers over VNC) that make it seem like a deliberate choice to obfuscate things -just- enough to make other tools difficult.\",\n            \"points\": null,\n            \"time\": \"2015-12-02T18:08:42.000Z\",\n            \"children\": [\n              {\n                \"id\": 10665420,\n                \"author\": \"SEJeff\",\n                \"text\": \"No a career building those types of tools taught me SoL is garbage from most vendors. Cray, Dell, and HP are (arguably) best with mostly reliable SoL, but they still are awful. If you paste a buffer too big into a SoL session, the dell DRAC will freeze, so you have to kill and restart the serial connection. If you have > 1000 machines, hardware serial is the best thing to do for management, in addition to IPMI for power management.\",\n                \"points\": null,\n                \"time\": \"2015-12-02T20:03:11.000Z\",\n                \"children\": []\n              }\n            ]\n          }\n        ]\n      }\n    ]\n  },\n  {\n    \"id\": 10661724,\n    \"author\": \"mverwijs\",\n    \"text\": \"We did something similar at Optiver. * boot a custom live cd (a la Knoppix) over PXE * Live CD places node into database if it doesn't exist yet, using dmidecode to find serialnumbers and such * Live Cd keeps querying database for instructions * Engineer adds a profile to the node in the database * Live Cd slices up disk to match the profile * Live Cd fetches a tarball of the base OS from an URL and throws it on the metal. Runs grub setup. Reboots. Tumblr did something like this with http://tumblr.github.io/collins/.\",\n    \"points\": null,\n    \"time\": \"2015-12-02T08:37:08.000Z\",\n    \"children\": [\n      {\n        \"id\": 10662262,\n        \"author\": \"defect\",\n        \"text\": \"Also checkout https://github.com/Tumblr/genesis which does sort of the same thing as the PXE image GitHub writes about.\",\n        \"points\": null,\n        \"time\": \"2015-12-02T11:08:10.000Z\",\n        \"children\": []\n      },\n      {\n        \"id\": 10662370,\n        \"author\": \"voltagex_\",\n        \"text\": \"Hey cool so what I'm fiddling around with in iPXE isn't completely obsolete in the age of Docker and disposable VMs.\",\n        \"points\": null,\n        \"time\": \"2015-12-02T11:41:05.000Z\",\n        \"children\": [\n          {\n            \"id\": 10662753,\n            \"author\": \"mverwijs\",\n            \"text\": \"Well, no. But IMHO, iron is only do-able if you have enough of it. This I learned the hard way at my previous contract, where we only had a handful of servers and all of them production.When you want to automate your complete infra, including rolling out hardware, you need hardware to develop and test on. Entropy of life will ensure that exactly that moment you need to reinstall that PostgreSQL slave from scratch, the PXE server is unreachable, or the server has a different diskcontroller, or an iLO certificate expired. Or something stupid.Test your code. And for Ops this means: machines that are solely for the testing pleasure of Ops. No other function.\",\n            \"points\": null,\n            \"time\": \"2015-12-02T13:20:25.000Z\",\n            \"children\": [\n              {\n                \"id\": 10666824,\n                \"author\": \"voltagex_\",\n                \"text\": \"In a perfect world you'd have a Dev, Staging and Prod PXE server, but reality says you're not going to be able to get signoff to run that many.\",\n                \"points\": null,\n                \"time\": \"2015-12-03T00:03:25.000Z\",\n                \"children\": [\n                  {\n                    \"id\": 10667018,\n                    \"author\": \"mverwijs\",\n                    \"text\": \"But that is perfectly fine! That only means that you cannot have an automated install procedure that the company can rely on. There is really nothing wrong with a little manual labour at this stage. Do not spend weeks and weeks on automating this without having the environment to test your code.\",\n                    \"points\": null,\n                    \"time\": \"2015-12-03T00:54:50.000Z\",\n                    \"children\": []\n                  }\n                ]\n              }\n            ]\n          },\n          {\n            \"id\": 10662830,\n            \"author\": \"vidarh\",\n            \"text\": \"Even those machines running Docker and disposable VMs still need to run on something.I've got systems PXE booting to CoreOS where everything else runs in Docker containers (even the odd KVM VM).\",\n            \"points\": null,\n            \"time\": \"2015-12-02T13:40:55.000Z\",\n            \"children\": []\n          }\n        ]\n      },\n      {\n        \"id\": 10662731,\n        \"author\": \"mpdehaan2\",\n        \"text\": \"I did something sort of similar with Live CD images for booting hardware not on a PXE network in Cobbler. It could also register machines seen then first time by making system records for them.https://github.com/cobbler/cobblerWhat is interesting in this article in particular are the auto-firmware upgrade state transitions, which seem pretty neat.The chat bot is I guess neat, but if a lot of people are using the cloud that could be a hard way to get status.\",\n        \"points\": null,\n        \"time\": \"2015-12-02T13:17:20.000Z\",\n        \"children\": []\n      },\n      {\n        \"id\": 10662755,\n        \"author\": \"invertedohm\",\n        \"text\": \"Kind of amazing that the state of the art in this area hasn't changed in nearly 10 years. The marketing angle is funny as well - everything has to be a cloud now. I think I used to call what we built at Optiver a \\\"private cloud\\\" but \\\"Metal Cloud\\\" is nicely buzzwordy as well.PS - Hi Marty, unknown nick here but I'm sure you can figure out who I am. :D\",\n        \"points\": null,\n        \"time\": \"2015-12-02T13:20:28.000Z\",\n        \"children\": []\n      },\n      {\n        \"id\": 10662800,\n        \"author\": \"jsjohnst\",\n        \"text\": \"Actually there's heavy overlap in functionality if you include other Tumblr things with Collins like Phil, Genesis, and Configrr.Collins - asset managementPhil - ipxe booting based on Collins stateGenesis - base hardware configuration (firmware, bios, raid, etc), burnin, kickstartConfigrr - state / config management\",\n        \"points\": null,\n        \"time\": \"2015-12-02T13:32:02.000Z\",\n        \"children\": []\n      }\n    ]\n  },\n  {\n    \"id\": 10661802,\n    \"author\": \"seanhandley\",\n    \"text\": \"Sounds a lot like http://theforeman.org/\",\n    \"points\": null,\n    \"time\": \"2015-12-02T08:59:28.000Z\",\n    \"children\": []\n  },\n  {\n    \"id\": 10661940,\n    \"author\": \"A_Beer_Clinked\",\n    \"text\": \"Can anybody with experience of using Openstack Ironic[1] in this space comment on advantages of rolling your own liek GitHub?[1] https://wiki.openstack.org/wiki/Ironic\",\n    \"points\": null,\n    \"time\": \"2015-12-02T09:46:24.000Z\",\n    \"children\": [\n      {\n        \"id\": 10662084,\n        \"author\": \"detaro\",\n        \"text\": \"If you are not running OpenStack otherwise I'd say maintaining it is way to much effort to get relatively basic functionality. They also would still need custom work for asset databases, the memory checks, so they don't gain much.\",\n        \"points\": null,\n        \"time\": \"2015-12-02T10:22:28.000Z\",\n        \"children\": []\n      },\n      {\n        \"id\": 10663575,\n        \"author\": \"mariojv\",\n        \"text\": \"The OnMetal [1] team at Rackspace uses OpenStack Ironic.It's saved us a lot of engineering time and let us offer the same interface as our VMs for provisioning baremetal machines with OpenStack Nova.[1] http://www.rackspace.com/en-us/cloud/servers/onmetal\",\n        \"points\": null,\n        \"time\": \"2015-12-02T15:42:36.000Z\",\n        \"children\": []\n      },\n      {\n        \"id\": 10663813,\n        \"author\": \"dmourati\",\n        \"text\": \"A consultant from Ubuntu called Ironic a \\\"still birth\\\" and said its name was indicative of its fit with the rest of OpenStack. His team used MaaS which I gather serves most of the same purpose:https://maas.ubuntu.com/\",\n        \"points\": null,\n        \"time\": \"2015-12-02T16:16:16.000Z\",\n        \"children\": [\n          {\n            \"id\": 10665215,\n            \"author\": \"stephenr\",\n            \"text\": \"Breaking news: A consultant for a company trash talks an open source competitor.\",\n            \"points\": null,\n            \"time\": \"2015-12-02T19:29:52.000Z\",\n            \"children\": []\n          }\n        ]\n      }\n    ]\n  },\n  {\n    \"id\": 10661983,\n    \"author\": \"stephenr\",\n    \"text\": \"These lines seem odd to me, maybe it's just the wording:> [gPanel] Deploying DNS via Heaven... > hubot is deploying dns/master (deadbeef) to production. > hubot's production deployment of dns/master (deadbeef) is done! (6s)Is this just an IMO odd use of the word \\\"deploying\\\" or does a DNS change really mean building and deploying a new package/image?\",\n    \"points\": null,\n    \"time\": \"2015-12-02T09:56:57.000Z\",\n    \"children\": [\n      {\n        \"id\": 10662011,\n        \"author\": \"jcrawfordor\",\n        \"text\": \"I've always thought of 'deploy' as a generic term for pushing any change to production. I think this is pretty typical - e.g. you deploy your application, even if that just means updating some files.\",\n        \"points\": null,\n        \"time\": \"2015-12-02T10:03:09.000Z\",\n        \"children\": []\n      },\n      {\n        \"id\": 10662043,\n        \"author\": \"_yy\",\n        \"text\": \"They probably manage DNS in a Git repository/using Puppet, so deploying may be quite literal. I see no issue with that.\",\n        \"points\": null,\n        \"time\": \"2015-12-02T10:13:20.000Z\",\n        \"children\": [\n          {\n            \"id\": 10662055,\n            \"author\": \"mrmondo\",\n            \"text\": \"We do this, it works well do us with 200-300 servers.\",\n            \"points\": null,\n            \"time\": \"2015-12-02T10:16:04.000Z\",\n            \"children\": []\n          }\n        ]\n      },\n      {\n        \"id\": 10662208,\n        \"author\": \"vidarh\",\n        \"text\": \"I deploy public DNS via new images. Image builds are fast, and our DNS changes rarely. When DNS changes frequently I wouldn't recommend it, though. Our internal DNS is using SkyDNS2 (backed by Etcd) instead, because that changes frequently (service registrations etc. as vms are started/stopped). But for the public DNS we like having the one DNS change => one git commit => one Docker image mapping to see who/why/when DNS changed.\",\n        \"points\": null,\n        \"time\": \"2015-12-02T10:52:10.000Z\",\n        \"children\": []\n      },\n      {\n        \"id\": 10663540,\n        \"author\": \"samlambert\",\n        \"text\": \"We can deploy DNS like an app. If we want to add hostnames manually we do it in git and deploy the DNS \\\"app\\\" via hubot.\",\n        \"points\": null,\n        \"time\": \"2015-12-02T15:37:00.000Z\",\n        \"children\": []\n      }\n    ]\n  },\n  {\n    \"id\": 10661994,\n    \"author\": \"castell\",\n    \"text\": \"The Hubot workflow sounds interesting. It seems more and more DevOps prefer it.Has someone first hand experience with such Hubot usage? Do you prefer such commands or would you want to write more informal short sentences?\",\n    \"points\": null,\n    \"time\": \"2015-12-02T10:00:12.000Z\",\n    \"children\": [\n      {\n        \"id\": 10662009,\n        \"author\": \"Rockslide\",\n        \"text\": \"There was also a talk titled \\\"Chatops\\\" about this specific thing if you want to learn more: https://www.youtube.com/watch?v=NST3u-GjjFw\",\n        \"points\": null,\n        \"time\": \"2015-12-02T10:02:53.000Z\",\n        \"children\": []\n      },\n      {\n        \"id\": 10662082,\n        \"author\": \"jon-wood\",\n        \"text\": \"I'm somewhat split. There's definite value in having a shared history of what people have done, but I prefer that to take the form of command line tools pushing status updates to Hipchat or whatever. You lose so much convineance by pretending Hipchat's chat box is a terminal, everything from command history to being able to quickly iterate over the contents of a file or set environment variables.\",\n        \"points\": null,\n        \"time\": \"2015-12-02T10:22:12.000Z\",\n        \"children\": [\n          {\n            \"id\": 10662236,\n            \"author\": \"glenngillen\",\n            \"text\": \"One of the most compelling aspects of it for me isn't so much the shared history, but just visibility of what people have done. It's a really effective way of transferring knowledge of how things are done. You can easily drop into the room and watch play-by-play how a given task is done.\",\n            \"points\": null,\n            \"time\": \"2015-12-02T10:59:03.000Z\",\n            \"children\": [\n              {\n                \"id\": 10663415,\n                \"author\": \"greggyb\",\n                \"text\": \"If this is the goal, why not just have a script export the shell's history to a chat channel?To me, a wiki would be even better, because you could retroactively include expository comments along with the command history.\",\n                \"points\": null,\n                \"time\": \"2015-12-02T15:18:48.000Z\",\n                \"children\": []\n              }\n            ]\n          }\n        ]\n      },\n      {\n        \"id\": 10662338,\n        \"author\": \"ktt\",\n        \"text\": \"Unfortunately most of the material on ChatOps currently covers only how to get Hubot to display cat pictures or other trivia [1]. Maybe it's because each company should create their own \\\"chat API\\\" but I'd also like to hear some real, inside \\\"war stories\\\".Does anyone knows what app does GitHub use for chats? Looks like a simple and elegant UI over Basecamp.[1]: http://hubot-script-catalog.herokuapp.com/\",\n        \"points\": null,\n        \"time\": \"2015-12-02T11:29:09.000Z\",\n        \"children\": [\n          {\n            \"id\": 10662991,\n            \"author\": \"tehbeard\",\n            \"text\": \"Going off Hubot's sourcecode, I'd guess campfire.\",\n            \"points\": null,\n            \"time\": \"2015-12-02T14:13:26.000Z\",\n            \"children\": []\n          }\n        ]\n      },\n      {\n        \"id\": 10663755,\n        \"author\": \"q3k\",\n        \"text\": \"Semi-off-topic, but I am genuinely scared by giving a chat bot full root access to your infrastructure. This just doesn't seem like a mature enough, AAA-enabled channel. Especially when there's a third party (HipChat/Slack/...) involved.\",\n        \"points\": null,\n        \"time\": \"2015-12-02T16:08:17.000Z\",\n        \"children\": []\n      }\n    ]\n  },\n  {\n    \"id\": 10662233,\n    \"author\": \"ymse\",\n    \"text\": \"Hardware provisioning is a dying art. I would love to see a modern-day xCAT[0] clone that's easy to install and configure and with proper multi-platform support. Foreman is half-way there, but AFAIK doesn't do BMC provisioning and discovery, which is a big deal.0: https://github.com/xcat2/xcat-core/blob/master/docs/source/i...\",\n    \"points\": null,\n    \"time\": \"2015-12-02T10:58:04.000Z\",\n    \"children\": [\n      {\n        \"id\": 10663679,\n        \"author\": \"eLobato\",\n        \"text\": \"Foreman does discovery [1] (PXE, PXE-less, segmented networks, through bootdisk) and handles BMC, I wrote the API in fact :)http://theforeman.org/plugins/foreman_discovery/4.1/index.ht...Maybe you were not aware because it's a plugin, we kind of have that problem in the Foreman community, plugins are not as visible as they should and they can contain key features.\",\n        \"points\": null,\n        \"time\": \"2015-12-02T15:57:06.000Z\",\n        \"children\": []\n      }\n    ]\n  },\n  {\n    \"id\": 10662761,\n    \"author\": \"dorfsmay\",\n    \"text\": \"Not as detailled on the tooling, but it sows how much hardware they use, Stackoverflow did a blog post on their datacentres move:http://blog.serverfault.com/2015/03/\",\n    \"points\": null,\n    \"time\": \"2015-12-02T13:21:57.000Z\",\n    \"children\": []\n  },\n  {\n    \"id\": 10662906,\n    \"author\": \"ptype\",\n    \"text\": \"Why would a company like GitHub choose Ubuntu over Debian? The LTS policy?\",\n    \"points\": null,\n    \"time\": \"2015-12-02T13:56:01.000Z\",\n    \"children\": [\n      {\n        \"id\": 10662932,\n        \"author\": \"wtbob\",\n        \"text\": \"I too would be interested in the answer. From my perspective, Debian is the server Linux distro par excellence, and in my experience the folks who choose Ubuntu have been devs who don't actually use Linux (e.g., the sorts who develop on a Mac or in a VM rather than on a personal Linux system). It's not really fair to Ubuntu, which is decent enough in its own way, but I tend to consider the choice of Ubuntu to be a bit of an architecture smell.I'm honestly interested in what the valid reasons to prefer Ubuntu over Debian (particularly on the server side) are.\",\n        \"points\": null,\n        \"time\": \"2015-12-02T14:01:18.000Z\",\n        \"children\": [\n          {\n            \"id\": 10663166,\n            \"author\": \"stephenr\",\n            \"text\": \"I agree with the main point you're trying to make, but the suggested examples you provide don't always hold true.I'm pretty dedicated to Debian on the server, a good part of my business is providing infrastructure support and setup, and I work from a MacBook Pro.\",\n            \"points\": null,\n            \"time\": \"2015-12-02T14:42:38.000Z\",\n            \"children\": []\n          },\n          {\n            \"id\": 10663254,\n            \"author\": \"avar\",\n            \"text\": \"Some choose it because Ubuntu, unlike Debian, has a guaranteed timely release cycle, and unlike Debian they have formalized LTS.\",\n            \"points\": null,\n            \"time\": \"2015-12-02T14:55:30.000Z\",\n            \"children\": [\n              {\n                \"id\": 10664114,\n                \"author\": \"Jgrubb\",\n                \"text\": \"This is why I choose Ubuntu.\",\n                \"points\": null,\n                \"time\": \"2015-12-02T17:00:42.000Z\",\n                \"children\": []\n              }\n            ]\n          },\n          {\n            \"id\": 10663742,\n            \"author\": \"q3k\",\n            \"text\": \"Newer packages, sane LTS policy, easier to get non-free firmware/drivers going (as in, the default CD comes with them), seemingly more support from third parties.They're both pretty awful due to their automagic(al tendency to break down in mysterious ways), but if I have to choose, I'll go with Ubuntu.Disclaimer: My main box runs Gentoo and I own no Mac machines, if that changes anything in your vision of Ubuntu users.\",\n            \"points\": null,\n            \"time\": \"2015-12-02T16:06:01.000Z\",\n            \"children\": [\n              {\n                \"id\": 10664383,\n                \"author\": \"zeveb\",\n                \"text\": \"> Newer packages, sane LTS policyThose two are in opposition: Debian (generally) has new-enough packages, but it's stable, which is what one wants on a server system. Meanwhile, Debian's LTS story is better than Ubuntu's: just upgrade, and know it will work.> easier to get non-free firmware/driversBut how often is that needed for server systems? And of course, there're the ethical & engineering issues with using proprietary software in the first place.> seemingly more support from third partiesThere is that, but if we all wanted more support from third parties, we'd have stuck with Windows, no?> They're both pretty awful due to their automagic(al tendency to break down in mysterious ways)I've not experienced that with Debian in a long time. I used to have issues with Ubuntu, but I don't think that they were generally all that bad. Better than what I used to experience with Macs and Windows back in the 90s, anyway.\",\n                \"points\": null,\n                \"time\": \"2015-12-02T17:32:04.000Z\",\n                \"children\": [\n                  {\n                    \"id\": 10664740,\n                    \"author\": \"FireBeyond\",\n                    \"text\": \"\\\"stable\\\". \\\"new-enough\\\"This is a leading word. As is a lot of that paragraph. For many tools some companies use, Debian certainly is NOT \\\"new-enough\\\" with many package choices. Nor is Ubuntu inherently NOT \\\"stable\\\" - and still trails a little behind the leading edge. As for upgrades, I've watched many a server upgrade seamlessly from 10.04 LTS to 12.04 to 14.04. I'm sure there can be and has been many a person, many a thread who've not had seamless experiences. But the same applies for Debian - heck, even the release manual has a section entitled \\\"How To Recover A Broken System\\\" with reference to system upgrades.\\\"non-free firmware/drivers\\\"How often needed? In this article alone, IPMI, BMC, RAID, BIOS.\\\"And of course, there're the ethical & engineering issues\\\"This is a derailment. What exactly are the ethical issues for a closed source company in using other proprietary software?I'm by no means an Ubuntu fanatic. It has its share of issues, absolutely. I have everything from FreeBSD to Debian to RHEL to OmniOS to administer, and they all have strengths and weaknesses.\",\n                    \"points\": null,\n                    \"time\": \"2015-12-02T18:21:06.000Z\",\n                    \"children\": []\n                  }\n                ]\n              },\n              {\n                \"id\": 10664896,\n                \"author\": \"scott_karana\",\n                \"text\": \"Drivers are definitely a big deal on desktops, but for servers?...\",\n                \"points\": null,\n                \"time\": \"2015-12-02T18:44:06.000Z\",\n                \"children\": [\n                  {\n                    \"id\": 10682052,\n                    \"author\": \"q3k\",\n                    \"text\": \"There's a lot of exotic hardware on servers (enterprise RAID controllers, converged and 10GbE NICs, SAN HBAs, IPMI configuration utilities...).\",\n                    \"points\": null,\n                    \"time\": \"2015-12-05T15:04:09.000Z\",\n                    \"children\": []\n                  }\n                ]\n              }\n            ]\n          }\n        ]\n      },\n      {\n        \"id\": 10665379,\n        \"author\": \"otterley\",\n        \"text\": \"A better question, in my mind, is why they'd choose Ubuntu over CentOS or RHEL, since they're running on Dell hardware, and Kickstart is far, far superior to Debian/Ubuntu's PXE install (and preseeding is awful to configure). My org went through a lot of pain because of this choice (which preceded me) and I'd hate for anyone to go through what I went through.Also Dell's maintenance tooling barely works on Ubuntu at all; they don't even officially support their OpenManage stack on it. And forget about online firmware upgrades.\",\n        \"points\": null,\n        \"time\": \"2015-12-02T19:54:26.000Z\",\n        \"children\": [\n          {\n            \"id\": 10668967,\n            \"author\": \"stephenr\",\n            \"text\": \"> why they'd choose Ubuntu over CentOS or RHELMaybe they'd actually used them before.CentOS is a turd of a distribution, I'm certain the only reason it has any marketshare is because it's the only supported \\\"free\\\" OS for cPanel/WHM which a lot of web hosts provide for non-technical customers.\",\n            \"points\": null,\n            \"time\": \"2015-12-03T11:04:43.000Z\",\n            \"children\": [\n              {\n                \"id\": 10669441,\n                \"author\": \"Jgrubb\",\n                \"text\": \"I personally think it gets share with hosts because it never updates anything so it's less work for them to maintain (which I can't hate on).\",\n                \"points\": null,\n                \"time\": \"2015-12-03T13:34:40.000Z\",\n                \"children\": []\n              },\n              {\n                \"id\": 10672411,\n                \"author\": \"otterley\",\n                \"text\": \"> CentOS is a turd of a distributionWhat's the matter with it?\",\n                \"points\": null,\n                \"time\": \"2015-12-03T20:36:39.000Z\",\n                \"children\": []\n              }\n            ]\n          }\n        ]\n      }\n    ]\n  },\n  {\n    \"id\": 10663236,\n    \"author\": \"danesparza\",\n    \"text\": \"I've known about Hubot for a while.But did anybody else see Hubot with a Santa hat and think that was adorable? Because I did.\",\n    \"points\": null,\n    \"time\": \"2015-12-02T14:52:39.000Z\",\n    \"children\": []\n  },\n  {\n    \"id\": 10663788,\n    \"author\": \"grandalf\",\n    \"text\": \"This is pretty cool but is there really that much benefit to doing this? What size IT staff do the savings justify? Does that change if you use Amazon's market-driven options like spot pricing and capacity planning discounts?\",\n    \"points\": null,\n    \"time\": \"2015-12-02T16:12:53.000Z\",\n    \"children\": [\n      {\n        \"id\": 10664588,\n        \"author\": \"scott_karana\",\n        \"text\": \"The equivalent of a M4.x10large running 24/7, which would cost $1814/month, costs about $400-$500/month lease from Dell or HP.There are other costs, like cooling, power, peering, networking gear, colocation/building costs, having spare parts on hand, paying sysadmins, et cetera, that are going to vary based on your requirements and region.\",\n        \"points\": null,\n        \"time\": \"2015-12-02T17:59:25.000Z\",\n        \"children\": []\n      }\n    ]\n  },\n  {\n    \"id\": 10664299,\n    \"author\": \"amalag\",\n    \"text\": \"Is this also done remotely by gPanel?>Once we've gathered all the information we need about the machine, we enter configuring, where we assign a static IP address to the IPMI interface and tweak our BIOS settings. From there we move to firmware_upgrade where we update FCB, BMC, BIOS, RAID, and any other firmware we'd like to manage on the system.\",\n    \"points\": null,\n    \"time\": \"2015-12-02T17:23:56.000Z\",\n    \"children\": [\n      {\n        \"id\": 10664675,\n        \"author\": \"FireBeyond\",\n        \"text\": \"In theory it should be if you have a tightly controlled hardware process (and in this case, Dell, who is used to selling servers configured to initially PXE boot, etc), and you have some 'expect/send' scripting in place.\",\n        \"points\": null,\n        \"time\": \"2015-12-02T18:12:26.000Z\",\n        \"children\": [\n          {\n            \"id\": 10665655,\n            \"author\": \"amalag\",\n            \"text\": \"I was not aware you can control BIOS settings, BIOS upgrades and configuring IPMI remotely like that.EDIT - looks like it is straightforward if you control the IPMI locally. So the software would send commands to do it locally.\",\n            \"points\": null,\n            \"time\": \"2015-12-02T20:43:19.000Z\",\n            \"children\": []\n          }\n        ]\n      }\n    ]\n  }\n]",
  "meta.json": "{\n  \"rank\": 4,\n  \"title\": \"GitHub's Metal Cloud\",\n  \"url\": \"http://githubengineering.com/githubs-metal-cloud/\",\n  \"hn_url\": \"https://news.ycombinator.com/item?id=10657366\",\n  \"points\": 205,\n  \"author\": \"samlambert\",\n  \"comment_count\": 80\n}"
}